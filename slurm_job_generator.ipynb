{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_sbatch_fmri(\n",
    "    job_name='brainnat',\n",
    "    hour=48,\n",
    "    minute=00,\n",
    "    gpu='',\n",
    "    constraint=\"a100|h100\",\n",
    "    output_dir_base='/scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/',\n",
    "    script_name='Train.py',\n",
    "    num_gpus=1,\n",
    "    batch_size=21,\n",
    "    model_name=\"multisubject_nat\",\n",
    "    data_path='/scratch/cl6707/Shared_Datasets/NSD_MindEye/Mindeye2',\n",
    "    cache_dir='/scratch/cl6707/Shared_Datasets/NSD_MindEye/Mindeye2',\n",
    "    multi_subject=\"1,2,5,7\",\n",
    "    subj=1,\n",
    "    max_lr=3e-4,\n",
    "    mixup_pct=0.33,\n",
    "    num_epochs=150,\n",
    "    use_prior=True,\n",
    "    prior_scale=30,\n",
    "    clip_scale=1,\n",
    "    blurry_recon=False,\n",
    "    blur_scale=0.5,\n",
    "    use_image_aug=False,\n",
    "    n_blocks=4,\n",
    "    hidden_dim=512,\n",
    "    num_sessions=40,\n",
    "    ckpt_interval=999,\n",
    "    ckpt_saving=True,\n",
    "    wandb_log=True,\n",
    "    num_heads=4,\n",
    "    tome_r=2000,\n",
    "    last_n_features=16,\n",
    "    nat_depth=2,\n",
    "    nat_num_neighbors=8,\n",
    "    lr_scheduler_type='cycle',\n",
    "    seed=42,\n",
    "    use_mixer=False,\n",
    "    new_test=True,\n",
    "    wandb_project=\"BRAIN_NAT\",\n",
    "    multisubject_ckpt=None,\n",
    "    full_attention=True,\n",
    "    additional_args=None):\n",
    "    \n",
    "    # Add current date to job name\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    job_name = f\"{job_name}_{current_date}\"\n",
    "    \n",
    "    # Start constructing the sbatch script\n",
    "    text = '#!/bin/bash\\n\\n'\n",
    "    text += f'#SBATCH --job-name={job_name}\\n'\n",
    "    text += '#SBATCH --nodes=1\\n'\n",
    "    text += '#SBATCH --cpus-per-task=16\\n'\n",
    "    text += '#SBATCH --mem=64GB\\n'\n",
    "    text += f'#SBATCH --time={hour}:{minute:02d}:00\\n'\n",
    "    text += f'#SBATCH --gres=gpu:{num_gpus}\\n'\n",
    "    text += f'#SBATCH --constraint=\"{constraint}\"\\n'\n",
    "    text += '#SBATCH --account=pr_60_tandon_advanced\\n\\n'\n",
    "\n",
    "    text += 'overlay_ext3=/scratch/cl6707/dl-env/fMRI.ext3\\n'\n",
    "    text += f'export NUM_GPUS={num_gpus}  # Set to equal gres=gpu:#!\\n'\n",
    "    text += f'export BATCH_SIZE={batch_size} # 21 for multisubject / 24 for singlesubject (orig. paper used 42 for multisubject / 24 for singlesubject)\\n'\n",
    "    text += 'export GLOBAL_BATCH_SIZE=$((BATCH_SIZE * NUM_GPUS))\\n\\n'\n",
    "\n",
    "    text += '# Make sure another job doesnt use same port, here using random number\\n'\n",
    "    text += 'export MASTER_PORT=$((RANDOM % (19000 - 11000 + 1) + 11000)) \\n'\n",
    "    text += 'export HOSTNAMES=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\")\\n'\n",
    "    text += 'export MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\\n'\n",
    "    text += 'export COUNT_NODE=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | wc -l)\\n'\n",
    "    text += 'echo MASTER_ADDR=${MASTER_ADDR}\\n'\n",
    "    text += 'echo MASTER_PORT=${MASTER_PORT}\\n'\n",
    "    text += 'echo WORLD_SIZE=${COUNT_NODE}\\n\\n'\n",
    "\n",
    "    text += 'singularity exec --nv \\\\\\n'\n",
    "    text += '    --overlay ${overlay_ext3}:ro \\\\\\n'\n",
    "    text += '    /scratch/work/public/singularity/cuda12.1.1-cudnn8.9.0-devel-ubuntu22.04.2.sif \\\\\\n'\n",
    "    text += '    /bin/bash -c \"\\n'\n",
    "    text += 'source /ext3/env.sh\\n'\n",
    "    text += 'cd /scratch/cl6707/Projects/fmri/Brain_Decoding/Downstream\\n\\n'\n",
    "    \n",
    "    text += 'export SSL_CERT_FILE=/scratch/cl6707/Shared_Datasets/cacert.pem\\n'\n",
    "    text += 'accelerate launch --num_processes=${NUM_GPUS} --main_process_port=${MASTER_PORT} --mixed_precision=fp16 Train.py\\\\\\n'\n",
    "    text += f'    --data_path={data_path} \\\\\\n'\n",
    "    text += f'    --cache_dir={cache_dir} \\\\\\n'\n",
    "    text += f'    --model_name={model_name}\\\\\\n'\n",
    "    text += f'    --multi_subject=\"{multi_subject}\" \\\\\\n'\n",
    "    text += f'    --subj={subj} \\\\\\n'\n",
    "    text += f'    --batch_size={batch_size} \\\\\\n'\n",
    "    text += f'    --max_lr={max_lr} \\\\\\n'\n",
    "    text += f'    --mixup_pct={mixup_pct} \\\\\\n'\n",
    "    text += f'    --num_epochs={num_epochs} \\\\\\n'\n",
    "    text += f'    {\"--use_prior\" if use_prior else \"--no-use_prior\"} \\\\\\n'\n",
    "    text += f'    --prior_scale={prior_scale} \\\\\\n'\n",
    "    text += f'    --clip_scale={clip_scale} \\\\\\n'\n",
    "    text += f'    {\"--blurry_recon\" if blurry_recon else \"--no-blurry_recon\"} \\\\\\n'\n",
    "    text += f'    --blur_scale={blur_scale} \\\\\\n'\n",
    "    text += f'    {\"--use_image_aug\" if use_image_aug else \"--no-use_image_aug\"} \\\\\\n'\n",
    "    text += f'    --n_blocks={n_blocks} \\\\\\n'\n",
    "    text += f'    --hidden_dim={hidden_dim} \\\\\\n'\n",
    "    text += f'    --num_sessions={num_sessions} \\\\\\n'\n",
    "    text += f'    --ckpt_interval={ckpt_interval} \\\\\\n'\n",
    "    text += f'    {\"--ckpt_saving\" if ckpt_saving else \"--no-ckpt_saving\"} \\\\\\n'\n",
    "    text += f'    {\"--wandb_log\" if wandb_log else \"--no-wandb_log\"} \\\\\\n'\n",
    "    text += f'    --num_heads={num_heads} \\\\\\n'\n",
    "    text += f'    --tome_r={tome_r} \\\\\\n'\n",
    "    text += f'    --last_n_features={last_n_features} \\\\\\n'\n",
    "    text += f'    --nat_depth={nat_depth} \\\\\\n'\n",
    "    text += f'    --nat_num_neighbors={nat_num_neighbors} \\\\\\n'\n",
    "    text += f'    --lr_scheduler_type={lr_scheduler_type} \\\\\\n'\n",
    "    text += f'    --seed={seed} \\\\\\n'\n",
    "    text += f'    {\"--use_mixer\" if use_mixer else \"--no-use_mixer\"} \\\\\\n'\n",
    "    text += f'    {\"--new_test\" if new_test else \"--no-new_test\"} \\\\\\n'\n",
    "    text += f'    --wandb_project={wandb_project} \\\\\\n'\n",
    "    text += f'    {\"--full_attention\" if full_attention else \"--no-full_attention\"} \\\\\\n'\n",
    "    text += '\"\\n'\n",
    "\n",
    "    # Save the sbatch script to a file\n",
    "    os.makedirs(output_dir_base, exist_ok=True)\n",
    "    job_file = os.path.join(output_dir_base, f'{job_name}.sbatch')\n",
    "    with open(job_file, 'w') as f:\n",
    "        f.write(text)\n",
    "    print(f'sbatch {job_file}')\n",
    "    return text\n",
    "\n",
    "def generate_ablation_jobs(base_params, param_ranges):\n",
    "    jobs = []\n",
    "    \n",
    "    # Generate all combinations of parameter values\n",
    "    param_names = list(param_ranges.keys())\n",
    "    param_values = list(param_ranges.values())\n",
    "    for values in itertools.product(*param_values):\n",
    "        params = base_params.copy()\n",
    "        for name, value in zip(param_names, values):\n",
    "            params[name] = value\n",
    "        \n",
    "        # Generate a unique job name\n",
    "        model_name = f\"{params['wandb_project']}_{'_'.join([f'{name}_{value}' for name, value in zip(param_names, values)])}\"\n",
    "        params['model_name'] = model_name+f\"_{datetime.now().strftime('%Y%m%d')}\"\n",
    "        job_name = model_name\n",
    "        # Generate the job script\n",
    "        job_script = generate_sbatch_fmri(\n",
    "            job_name=job_name,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        jobs.append((job_name, job_script))\n",
    "    \n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_16_nat_depth_3_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_16_nat_depth_4_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_16_nat_depth_5_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_16_nat_depth_6_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_32_nat_depth_3_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_32_nat_depth_4_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_32_nat_depth_5_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_500_last_n_features_32_nat_depth_6_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_16_nat_depth_3_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_16_nat_depth_4_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_16_nat_depth_5_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_16_nat_depth_6_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_32_nat_depth_3_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_32_nat_depth_4_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_32_nat_depth_5_nat_num_neighbors_16_20241106.sbatch\n",
      "sbatch /scratch/cl6707/Projects/fmri/Brain_Decoding/jobs/BRAIN_FAT_num_heads_8_tome_r_1000_last_n_features_32_nat_depth_6_nat_num_neighbors_16_20241106.sbatch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "base_params = {\n",
    "    \"script_name\": \"Train.py\",\n",
    "    \"data_path\": \"/scratch/cl6707/Shared_Datasets/NSD_MindEye/Mindeye2\",\n",
    "    \"cache_dir\": \"/scratch/cl6707/Shared_Datasets/NSD_MindEye/Mindeye2\",\n",
    "    \"multi_subject\": \"1,2,3,4,5,6,7\",\n",
    "    \"subj\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_lr\": 3e-4,\n",
    "    \"mixup_pct\": 0.33,\n",
    "    \"num_epochs\": 150,\n",
    "    \"use_prior\": False,\n",
    "    \"prior_scale\": 30,\n",
    "    \"clip_scale\": 1,\n",
    "    \"blurry_recon\": False,\n",
    "    \"blur_scale\": 0.5,\n",
    "    \"use_image_aug\": False,\n",
    "    \"n_blocks\": 4,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_sessions\": 40,\n",
    "    \"ckpt_interval\": 3,\n",
    "    \"ckpt_saving\": True,\n",
    "    \"wandb_log\": True,\n",
    "    \"lr_scheduler_type\": \"cycle\",\n",
    "    \"seed\": 42,\n",
    "    \"use_mixer\": False,\n",
    "    \"new_test\": True,\n",
    "    \"wandb_project\": \"BRAIN_FAT\",\n",
    "    \"full_attention\": True,\n",
    "}\n",
    "\n",
    "param_ranges = {\n",
    "    \"num_heads\": [8],\n",
    "    \"tome_r\": [500,1000],\n",
    "    \"last_n_features\": [16,32],\n",
    "    \"nat_depth\": [3,4,5,6],\n",
    "    \"nat_num_neighbors\": [16]\n",
    "}\n",
    "\n",
    "ablation_jobs = generate_ablation_jobs(base_params, param_ranges)\n",
    "\n",
    "# Print or save the generated job scripts\n",
    "# for job_name, job_script in ablation_jobs:\n",
    "    # print(f\"Generated job: {job_name}\")\n",
    "    # Uncomment the following lines to save each job script to a file\n",
    "    # with open(f\"{job_name}.sbatch\", \"w\") as f:\n",
    "    #     f.write(job_script)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "emo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
